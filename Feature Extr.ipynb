{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import math\n",
    "from PIL import Image\n",
    "from normalization import normalizeStaining\n",
    "import utilities\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageDraw\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import cv2\n",
    "import math\n",
    "from PIL import Image\n",
    "from normalization import normalizeStaining\n",
    "import utilities\n",
    "import sys\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "from PIL import Image, ImageDraw\n",
    "from xml.dom import minidom\n",
    "import lxml\n",
    "import colorsys\n",
    "import utilities\n",
    "import pandas as pd\n",
    "from skimage.morphology import watershed\n",
    "from skimage.feature import peak_local_max\n",
    "from skimage import morphology\n",
    "from scipy import ndimage as ndi\n",
    "from skimage.segmentation import mark_boundaries\n",
    "from IPython import display\n",
    "from collections import defaultdict\n",
    "from imgaug import augmenters as iaa\n",
    "from normalization import normalizeStaining\n",
    "%matplotlib inline \n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.getcwd()\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shape Intensity Texture feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shape \n",
    "def shape_feature(contours):\n",
    "    cnt=contours[0]\n",
    "    M = cv2.moments(cnt)\n",
    "    perimeter = cv2.arcLength(cnt,True)\n",
    "    area = cv2.contourArea(cnt)\n",
    "    (x, y), radius = cv2.minEnclosingCircle(cnt)    \n",
    "    X,Y,w,h = cv2.boundingRect(cnt)    \n",
    "    rect_area = w*h\n",
    "    RspectRatio = float(w)/h\n",
    "    #8.extent\n",
    "    extent = float(area)/rect_area\n",
    "    #19\n",
    "    if (len(cnt))>=5:\n",
    "        rect= cv2.minAreaRect(cnt)\n",
    "        width,height=rect[1]\n",
    "        minminAreaRect=width*height\n",
    "    #20.Rectangularity\n",
    "        if minminAreaRect>0:\n",
    "            Rectangularity=float(area)/minminAreaRect\n",
    "        else:\n",
    "            Rectangularity='NAN'\n",
    "            \n",
    "    #9.\n",
    "        hull = cv2.convexHull(cnt)\n",
    "        hull_area = cv2.contourArea(hull)\n",
    "        solidity = float(area)/hull_area\n",
    "        (xx,yy),(MA,ma),angle = cv2.fitEllipse(cnt)\n",
    "        ellarea=np.pi*ma*MA/4\n",
    "    #18.Eccentricity\n",
    "        Eccentricity = MA/ma\n",
    "    else:\n",
    "        minminAreaRect='NAN'\n",
    "        Rectangularity='NAN'\n",
    "        hull_area ='NAN'\n",
    "    #10.solidity \n",
    "        solidity='NAN'\n",
    "        MA='NAN'\n",
    "        ma='NAN'\n",
    "        angle='NAN'\n",
    "        ellarea='NAN'\n",
    "        Eccentricity='NAN'\n",
    "    #11.\n",
    "    equi_diameter = np.sqrt(4*area/np.pi)\n",
    "    #16.Perimeter to Surface ratio \n",
    "    paRatio = perimeter/area\n",
    "    #17.Roundness\n",
    "    Roundness = area/(np.pi*radius*radius)\n",
    "    #18.Compactness\n",
    "    if area>0:\n",
    "        Compactness = ((perimeter* perimeter)/(4*np.pi*area))\n",
    "    else:\n",
    "        Compactness=\"NAN\"\n",
    "    return perimeter,area,x,y,radius, rect_area,RspectRatio,extent,minminAreaRect,Rectangularity,hull_area ,solidity,equi_diameter,MA,ma,angle,ellarea,paRatio,Roundness,Compactness,Eccentricity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intensity feature(15):16\n",
    "def Intensity_feature(image):\n",
    "    #image is Colored single cell image\n",
    "    img_grays = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    imgray_hist = cv2.calcHist([img_grays],[0],None,[256],[0,255])\n",
    "    img_max = np.max(img_grays)\n",
    "    img_mean = np.mean(img_grays)\n",
    "    img_median = np.median(img_grays)\n",
    "    img_sd = np.std(img_grays)\n",
    "    img_Var = np.var(img_grays)\n",
    "    img_min =1000\n",
    "    img_range = 0\n",
    "    img_MAD = 0\n",
    "    img_Skewness = 0\n",
    "    img_Kurtosis = 0 \n",
    "    img_energy = 0\n",
    "    n = 0\n",
    "    means=0\n",
    "    stds=0\n",
    "    mean=0\n",
    "    for i in range(256):\n",
    "        for j in range(256):\n",
    "            img_energy += img_grays[i][j]**2\n",
    "            n += 1\n",
    "#             print(img_grays[i][j])\n",
    "            if img_grays[i][j]>=0 and img_grays[i][j]<img_min:\n",
    "                img_min = img_grays[i][j]\n",
    "            img_MAD += abs(img_grays[i][j] - img_mean)\n",
    "            means += (img_grays[i][j] - img_mean)**3\n",
    "            stds += (img_grays[i][j] - img_mean)**2\n",
    "            mean += (img_grays[i][j] - img_mean)**4\n",
    "    means/=n\n",
    "    stds/=n\n",
    "    mean/=n\n",
    "    stds=stds**(3/2)\n",
    "    img_Skewness = means/stds\n",
    "    img_Kurtosis = mean/stds**2\n",
    "    img_range = img_max-img_min\n",
    "    img_MAD/=n\n",
    "    \n",
    "    P_sum = 0\n",
    "    H_mean= 0\n",
    "    H_var= 0\n",
    "    Hist_Skewness=0\n",
    "    Hist_Kurtosis=0\n",
    "    Hist_Energy=0\n",
    "    Hist_Entropy=0\n",
    "    for i in range(256):\n",
    "        P_sum += imgray_hist[i]\n",
    "    for i in range(256):    \n",
    "        p = imgray_hist[i]/P_sum\n",
    "        Hist_Energy += p**2\n",
    "        H_mean += i*p\n",
    "    for i in range(256):    \n",
    "        p = imgray_hist[i]/P_sum\n",
    "        H_var += (i-H_mean)**2*p\n",
    "        Hist_Skewness+=(i-H_mean)**3*p\n",
    "        Hist_Kurtosis+=(i-H_mean)**4*p\n",
    "    Hist_Skewness/= H_var**(3/2)\n",
    "    Hist_Kurtosis/= H_var**2\n",
    "    return img_max,img_min,img_mean,img_median,img_sd,img_Var,img_range,img_MAD,img_Skewness,img_Kurtosis,img_energy,H_mean[0],H_var[0],Hist_Skewness[0],Hist_Kurtosis[0],Hist_Energy[0]\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path=\"C:\\\\Users\\\\Yi\\\\Documents\\\\python_projects\\\\nucleusSeg\\\\vals_data\\\\0_12h (54)\\\\image.png\"\n",
    "image = cv2.imread(image_path)\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(image)\n",
    "a=Intensity_feature(image)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Texture\n",
    "from skimage.feature import greycomatrix,greycoprops\n",
    "def CLCM_feature(image):\n",
    "    result = greycomatrix(image, [1], [0, np.pi/4, np.pi/2, 3*np.pi/4],levels=256)\n",
    "    contrast = greycoprops(result,'contrast')\n",
    "    dissimilarity = greycoprops(result,'dissimilarity')\n",
    "    homogeneity = greycoprops(result,'homogeneity')\n",
    "    energy = greycoprops(result,'energy')\n",
    "    correlation = greycoprops(result,'correlation')\n",
    "    ASM = greycoprops(result,'ASM')\n",
    "  \n",
    "    contrast_m=np.mean(contrast)\n",
    "    dissimilarity_m=np.mean(dissimilarity)\n",
    "    homogeneity_m=np.mean(homogeneity)\n",
    "    energy_m=np.mean(energy)\n",
    "    correlation_m=np.mean(correlation)\n",
    "    ASM_m=np.mean(ASM)\n",
    "    contrast_s=np.std(contrast)\n",
    "    dissimilarity_s=np.std(dissimilarity)\n",
    "    homogeneity_s=np.std(homogeneity)\n",
    "    energy_s=np.std(energy)\n",
    "    correlation_s=np.std(correlation)\n",
    "    ASM_s=np.std(ASM)     \n",
    "    return contrast_m,dissimilarity_m,homogeneity_m,energy_m,correlation_m,ASM_m,contrast_s,dissimilarity_s,homogeneity_s,energy_s,correlation_s,ASM_s   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "def getGrayLevelRumatrix( array, theta):\n",
    "        P = array\n",
    "        x, y = P.shape\n",
    "        min_pixels = np.min(P)   \n",
    "        run_length = max(x, y)   \n",
    "        num_level = np.max(P) - np.min(P) + 1   \n",
    " \n",
    "        deg0 = [val.tolist() for sublist in np.vsplit(P, x) for val in sublist]   \n",
    "        deg90 = [val.tolist() for sublist in np.split(np.transpose(P), y) for val in sublist]   \n",
    "        diags = [P[::-1, :].diagonal(i) for i in range(-P.shape[0]+1, P.shape[1])]  \n",
    "        deg45 = [n.tolist() for n in diags]\n",
    "        Pt = np.rot90(P, 3)   \n",
    "        diags = [Pt[::-1, :].diagonal(i) for i in range(-Pt.shape[0]+1, Pt.shape[1])]\n",
    "        deg135 = [n.tolist() for n in diags]\n",
    "        \n",
    "        def length(l):\n",
    "            if hasattr(l, '__len__'):\n",
    "                return np.size(l)\n",
    "            else:\n",
    "                i = 0\n",
    "                for _ in l:\n",
    "                    i += 1\n",
    "                return i\n",
    "        glrlm = np.zeros((num_level, run_length, len(theta)))  \n",
    "        for angle in theta:\n",
    "            for splitvec in range(0, len(eval(angle))):\n",
    "                flattened = eval(angle)[splitvec]\n",
    "                answer = []\n",
    "                for key, iter in itertools.groupby(flattened):   \n",
    "                    answer.append((key, length(iter)))   \n",
    "                for ansIndex in range(0, len(answer)):\n",
    "                    glrlm[int(answer[ansIndex][0]-min_pixels), int(answer[ansIndex][1]-1), theta.index(angle)] += 1   # 每次将统计像素值减去最小值就可以填入GLRLM矩阵中\n",
    "        return glrlm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_over_degree(function, x1, x2):\n",
    "    rows, cols, nums = x1.shape\n",
    "    result = np.ndarray((rows, cols, nums))\n",
    "    for i in range(nums):\n",
    "#         print(x1[:, :, i])\n",
    "        result[:, :, i] = function(x1[:, :, i], x2)\n",
    "#         print(result[:, :, i])\n",
    "    result[result == np.inf] = 0\n",
    "    result[np.isnan(result)] = 0\n",
    "    return result\n",
    "def calcuteIJ(rlmatrix):\n",
    "    gray_level, run_length, _ = rlmatrix.shape\n",
    "    I, J = np.ogrid[0:gray_level, 0:run_length]\n",
    "    return I, J+1\n",
    "def calcuteS(rlmatrix):\n",
    "    return np.apply_over_axes(np.sum, rlmatrix, axes=(0, 1))[0, 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Short Run Emphasis(SRE)\n",
    "def getShortRunEmphasis(rlmatrix):\n",
    "        I, J = calcuteIJ(rlmatrix)\n",
    "        numerator = np.apply_over_axes(np.sum, apply_over_degree(np.divide, rlmatrix, (J*J)), axes=(0, 1))[0, 0]\n",
    "        S = calcuteS(rlmatrix)\n",
    "        return numerator / S\n",
    "#Long Run Emphasis(LRE)\n",
    "def getLongRunEmphasis(rlmatrix):\n",
    "        I, J = calcuteIJ(rlmatrix)\n",
    "        numerator = np.apply_over_axes(np.sum, apply_over_degree(np.multiply, rlmatrix, (J*J)), axes=(0, 1))[0, 0]\n",
    "        S = calcuteS(rlmatrix)\n",
    "        return numerator / S\n",
    "#Gray Level Non-Uniformity(GLN)\n",
    "def getGrayLevelNonUniformity(rlmatrix):\n",
    "        G = np.apply_over_axes(np.sum, rlmatrix, axes=1)\n",
    "        numerator = np.apply_over_axes(np.sum, (G*G), axes=(0, 1))[0, 0]\n",
    "        S = calcuteS(rlmatrix)\n",
    "        return numerator / S\n",
    "# 4. RLN\n",
    "def getRunLengthNonUniformity(rlmatrix):\n",
    "    R = np.apply_over_axes(np.sum, rlmatrix, axes=0)\n",
    "    numerator = np.apply_over_axes(np.sum, (R*R), axes=(0, 1))[0, 0]\n",
    "    S = calcuteS(rlmatrix)\n",
    "    return numerator / S\n",
    " \n",
    "# 5. RP\n",
    "def getRunPercentage(rlmatrix):\n",
    "    gray_level, run_length, _ = rlmatrix.shape\n",
    "    num_voxels = gray_level * run_length\n",
    "    return calcuteS(rlmatrix) / num_voxels\n",
    " \n",
    "# 6. LGLRE\n",
    "def getLowGrayLevelRunEmphasis(rlmatrix):\n",
    "    I, J = calcuteIJ(rlmatrix)\n",
    "    numerator = np.apply_over_axes(np.sum, apply_over_degree(np.divide, rlmatrix, (I*I)), axes=(0, 1))[0, 0]\n",
    "    S = calcuteS(rlmatrix)\n",
    "    return numerator / S\n",
    " \n",
    "# 7. HGLRE\n",
    "def getHighGrayLevelRunEmphais(rlmatrix):\n",
    "    I, J = calcuteIJ(rlmatrix)\n",
    "    numerator = np.apply_over_axes(np.sum, apply_over_degree(np.multiply, rlmatrix, (I*I)), axes=(0, 1))[0, 0]\n",
    "    S = calcuteS(rlmatrix)\n",
    "    return numerator / S\n",
    " \n",
    "# 8. SRLGLE\n",
    "def getShortRunLowGrayLevelEmphasis(rlmatrix):\n",
    "    I, J = calcuteIJ(rlmatrix)\n",
    "    numerator = np.apply_over_axes(np.sum, apply_over_degree(np.divide, rlmatrix, (I*I*J*J)), axes=(0, 1))[0, 0]\n",
    "    S = calcuteS(rlmatrix)\n",
    "    return numerator / S\n",
    " \n",
    "# 9. SRHGLE\n",
    "def getShortRunHighGrayLevelEmphasis(rlmatrix):\n",
    "    I, J = calcuteIJ(rlmatrix)\n",
    "    temp = apply_over_degree(np.multiply, rlmatrix, (I*I))\n",
    "    numerator = np.apply_over_axes(np.sum,apply_over_degree(np.divide, temp, (J*J)), axes=(0, 1))[0, 0]\n",
    "    S = calcuteS(rlmatrix)\n",
    "    return numerator / S\n",
    " \n",
    "# 10. LRLGLE\n",
    "def getLongRunLow(rlmatrix):\n",
    "    I, J = calcuteIJ(rlmatrix)\n",
    "    temp = apply_over_degree(np.multiply, rlmatrix, (J*J))\n",
    "    numerator = np.apply_over_axes(np.sum, apply_over_degree(np.divide, temp, (J*J)), axes=(0, 1))[0, 0]\n",
    "    S = calcuteS(rlmatrix)\n",
    "    return numerator / S\n",
    " \n",
    "# 11. LRHGLE\n",
    "def getLongRunHighGrayLevelEmphais(rlmatrix):\n",
    "    I, J =calcuteIJ(rlmatrix)\n",
    "    numerator = np.apply_over_axes(np.sum, apply_over_degree(np.multiply, rlmatrix, (I*I*J*J)), axes=(0, 1))[0, 0]\n",
    "    S = calcuteS(rlmatrix)\n",
    "    return numerator / S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_h(hist_h,hist_s,hist_v):\n",
    "    #image is Colored single cell image  result\n",
    "\n",
    "    h_mean=np.mean(hist_h)\n",
    "    h_var=np.var(hist_h)\n",
    "    h_sd = np.std(hist_h)\n",
    "    hs=np.mean((hist_h-h_mean)**3/pow(h_sd,3))\n",
    "    hk=np.mean(((hist_h-h_mean)**4)/pow(h_var,2))  \n",
    "    h_max = np.max(hist_h)\n",
    "    h_median = np.median(hist_h)\n",
    "    h_sum =np.sum(hist_h)\n",
    "\n",
    "    s_mean=np.mean(hist_s)\n",
    "    s_var=np.var(hist_s)\n",
    "    s_sd = np.std(hist_s)\n",
    "    ss=np.mean((hist_s-s_mean)**3/pow(s_sd,3))\n",
    "    sk=np.mean(((hist_s-s_mean)**4)/pow(s_var,2))  \n",
    "    s_max = np.max(hist_s)\n",
    "    s_median = np.median(hist_s)\n",
    "    s_sum =np.sum(hist_s)    \n",
    "\n",
    "    v_mean=np.mean(hist_v)\n",
    "    v_var=np.var(hist_v)\n",
    "    v_sd = np.std(hist_v)\n",
    "    vs=np.mean((hist_v-v_mean)**3/pow(v_sd,3))\n",
    "    vk=np.mean(((hist_v-v_mean)**4)/pow(v_var,2))  \n",
    "    v_max = np.max(hist_v)\n",
    "    v_median = np.median(hist_v)\n",
    "    v_sum =np.sum(hist_v)\n",
    "    \n",
    "    \n",
    "    return h_mean,h_var,h_sd,hs,hk,h_max ,h_median,h_sum,s_mean,s_var,s_sd,ss,sk,s_max ,s_median,s_sum,v_mean,v_var,v_sd,vs,vk,v_max ,v_median,v_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_r(hist_r,hist_g,hist_b):\n",
    "    #image is Colored single cell image  result\n",
    "    r_mean=np.mean(hist_r)\n",
    "    r_var=np.var(hist_r)\n",
    "    r_sd = np.std(hist_r)\n",
    "    rs=np.mean((hist_r-r_mean)**3/pow(r_sd,3))\n",
    "    rk=np.mean(((hist_r-r_mean)**4)/pow(r_var,2))  \n",
    "    r_max = np.max(hist_r)\n",
    "    r_median = np.median(hist_r)\n",
    "    r_sum =np.sum(hist_r)\n",
    "\n",
    "    g_mean=np.mean(hist_g)\n",
    "    g_var=np.var(hist_g)\n",
    "    g_sd = np.std(hist_g)\n",
    "    gs=np.mean((hist_g-g_mean)**3/pow(g_sd,3))\n",
    "    gk=np.mean(((hist_g-g_mean)**4)/pow(g_var,2))  \n",
    "    g_max = np.max(hist_g)\n",
    "    g_median = np.median(hist_g)\n",
    "    g_sum =np.sum(hist_g)    \n",
    "    \n",
    "    b_mean=np.mean(hist_b)\n",
    "    b_var=np.var(hist_b)\n",
    "    b_sd = np.std(hist_b)\n",
    "    bs=np.mean((hist_b-b_mean)**3/pow(b_sd,3))\n",
    "    bk=np.mean(((hist_b-b_mean)**4)/pow(b_var,2))  \n",
    "    b_max = np.max(hist_b)\n",
    "    b_median = np.median(hist_b)\n",
    "    b_sum =np.sum(hist_b)    \n",
    "    \n",
    "    return r_mean,r_var,r_sd,rs,rk,r_max ,r_median,r_sum,g_mean,g_var,g_sd,gs,gk,g_max ,g_median,g_sum,b_mean,b_var,b_sd,bs,bk,b_max ,b_median,b_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_l(l_hist,a_hist,b_hist):\n",
    "    #image is Colored single cell image  result\n",
    "    l_mean=np.mean(l_hist)\n",
    "    l_var=np.var(l_hist)\n",
    "    l_sd = np.std(l_hist)\n",
    "    ls=np.mean((l_hist-l_mean)**3/pow(l_sd,3))\n",
    "    lk=np.mean(((l_hist-l_mean)**4)/pow(l_var,2))  \n",
    "    l_max = np.max(l_hist)\n",
    "    l_median = np.median(l_hist)\n",
    "    l_sum =np.sum(l_hist)  \n",
    "\n",
    "    mean_a=np.mean(a_hist)\n",
    "    var_a=np.var(a_hist)\n",
    "    sd_a = np.std(a_hist)\n",
    "    s_a=np.mean((a_hist-mean_a)**3/pow(sd_a,3))\n",
    "    k_a=np.mean(((a_hist-mean_a)**4)/pow(var_a,2))  \n",
    "    max_a = np.max(a_hist)\n",
    "    median_a = np.median(a_hist)\n",
    "    sum_a =np.sum(a_hist)    \n",
    "    \n",
    "    mean_b=np.mean(b_hist)\n",
    "    var_b=np.var(b_hist)\n",
    "    sd_b = np.std(b_hist)\n",
    "    s_b=np.mean((b_hist-mean_b)**3/pow(sd_b,3))\n",
    "    k_b=np.mean(((b_hist-mean_b)**4)/pow(var_b,2))  \n",
    "    max_b = np.max(b_hist)\n",
    "    median_b = np.median(b_hist)\n",
    "    sum_b =np.sum(b_hist)        \n",
    "    \n",
    "    return  l_mean,l_var,l_sd,ls,lk,l_max ,l_median,l_sum,mean_a,var_a,sd_a,s_a,k_a,max_a,median_a, sum_a,mean_b,var_b,sd_b,s_b,k_b,max_b,median_b, sum_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "# from numba import jit\n",
    "np.set_printoptions(suppress=True) \n",
    "\n",
    "def glgcm(img_gray, ngrad=16, ngray=16):\n",
    "   \n",
    "    gsx = cv2.Sobel(img_gray, cv2.CV_64F, 1, 0, ksize=3)\n",
    "    gsy = cv2.Sobel(img_gray, cv2.CV_64F, 0, 1, ksize=3)\n",
    "    height, width = img_gray.shape\n",
    "    grad = (gsx ** 2 + gsy ** 2) ** 0.5 \n",
    "    grad = np.asarray(1.0 * grad * (ngrad-1) / grad.max(), dtype=np.int16)\n",
    "    gray = np.asarray(1.0 * img_gray * (ngray-1) / img_gray.max(), dtype=np.int16) # 0-255变换为0-15\n",
    "    gray_grad = np.zeros([ngray, ngrad]) \n",
    "    for i in range(height):\n",
    "        for j in range(width):\n",
    "            gray_value = gray[i][j]\n",
    "            grad_value = grad[i][j]\n",
    "            gray_grad[gray_value][grad_value] += 1\n",
    "    gray_grad = 1.0 * gray_grad / (height * width) \n",
    "    glgcm_features = get_glgcm_features(gray_grad)\n",
    "    return glgcm_features\n",
    "\n",
    "def get_glgcm_features(mat):\n",
    "    sum_mat = mat.sum()\n",
    "    small_grads_dominance = big_grads_dominance = gray_asymmetry = grads_asymmetry = energy = gray_mean = grads_mean = 0\n",
    "    gray_variance = grads_variance = corelation = gray_entropy = grads_entropy = entropy = inertia = differ_moment = 0\n",
    "    for i in range(mat.shape[0]):\n",
    "        gray_variance_temp = 0\n",
    "        for j in range(mat.shape[1]):\n",
    "            small_grads_dominance += mat[i][j] / ((j + 1) ** 2)\n",
    "            big_grads_dominance += mat[i][j] * j ** 2\n",
    "            energy += mat[i][j] ** 2\n",
    "            if mat[i].sum() != 0:\n",
    "                gray_entropy -= mat[i][j] * np.log(mat[i].sum())\n",
    "            if mat[:, j].sum() != 0:\n",
    "                grads_entropy -= mat[i][j] * np.log(mat[:, j].sum())\n",
    "            if mat[i][j] != 0:\n",
    "                entropy -= mat[i][j] * np.log(mat[i][j])\n",
    "                inertia += (i - j) ** 2 * np.log(mat[i][j])\n",
    "            differ_moment += mat[i][j] / (1 + (i - j) ** 2)\n",
    "            gray_variance_temp += mat[i][j] ** 0.5\n",
    "\n",
    "        gray_asymmetry += mat[i].sum() ** 2\n",
    "        gray_mean += i * mat[i].sum() ** 2\n",
    "        gray_variance += (i - gray_mean) ** 2 * gray_variance_temp\n",
    "    for j in range(mat.shape[1]):\n",
    "        grads_variance_temp = 0\n",
    "        for i in range(mat.shape[0]):\n",
    "            grads_variance_temp += mat[i][j] ** 0.5\n",
    "        grads_asymmetry += mat[:, j].sum() ** 2\n",
    "        grads_mean += j * mat[:, j].sum() ** 2\n",
    "        grads_variance += (j - grads_mean) ** 2 * grads_variance_temp\n",
    "    small_grads_dominance /= sum_mat\n",
    "    big_grads_dominance /= sum_mat\n",
    "    gray_asymmetry /= sum_mat\n",
    "    grads_asymmetry /= sum_mat\n",
    "    gray_variance = gray_variance ** 0.5\n",
    "    grads_variance = grads_variance ** 0.5\n",
    "    for i in range(mat.shape[0]):\n",
    "        for j in range(mat.shape[1]):\n",
    "            corelation += (i - gray_mean) * (j - grads_mean) * mat[i][j]\n",
    "    glgcm_features = [small_grads_dominance, big_grads_dominance, gray_asymmetry, grads_asymmetry, energy, gray_mean, grads_mean,\n",
    "        gray_variance, grads_variance, corelation, gray_entropy, grads_entropy, entropy, inertia, differ_moment]\n",
    "    return small_grads_dominance, big_grads_dominance, gray_asymmetry, grads_asymmetry, energy, gray_mean, grads_mean,gray_variance, grads_variance, corelation, gray_entropy, grads_entropy, entropy, inertia, differ_moment\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core Tumor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_xml_contour(xml_path, height, width):\n",
    "    '''\n",
    "    read contour from xml file and return mask\n",
    "    Args:\n",
    "        xml_path: full path of the xml file\n",
    "        height: height of the image\n",
    "        width: width of the image\n",
    "    Return:\n",
    "        mask: np array, shape=(height, width, num_instances), dtype=bool\n",
    "    '''\n",
    "    with open(xml_path, 'r') as fp:\n",
    "        soup = BeautifulSoup(fp.read(), 'xml')\n",
    "    contours = []\n",
    "    for contour in soup.find_all('Contour'):\n",
    "        points = []\n",
    "        for pt in contour.find_all('Pt'):\n",
    "            points.append(tuple(float(v) for v in pt.get_text().split(',')))\n",
    "        contours.append(points)\n",
    "    masks = []\n",
    "    for contour in contours:\n",
    "        img = Image.new('L', (width, height), 0)\n",
    "        ImageDraw.Draw(img).polygon(contour, outline=1, fill=1)\n",
    "        masks.append(np.array(img))   \n",
    "    mask = np.stack(masks, axis=-1)\n",
    "    return mask.astype(bool) # mask should be boolean type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xml_contour(xml_path):\n",
    "    with open(xml_path, 'r') as fp:\n",
    "        soup = BeautifulSoup(fp.read(), 'xml')\n",
    "    contours = []\n",
    "    for contour in soup.find_all('Contour'):\n",
    "        points = []\n",
    "        for pt in contour.find_all('Pt'):\n",
    "            points.append(tuple(float(v) for v in pt.get_text().split(',')))\n",
    "        contours.append(points)\n",
    "    return contours\n",
    "\n",
    "def change_number(contours,x,y):\n",
    "    W=1024\n",
    "    H=1024\n",
    "    contour=[]\n",
    "    for i in range(len(contours)):\n",
    "        c=[]\n",
    "        for j in range(len(contours[i])):\n",
    "            a= contours[i][j][0]+x*W\n",
    "            b= contours[i][j][1]+y*H\n",
    "            c.append((a,b))\n",
    "        contour.append(c) \n",
    "    return contour    \n",
    "\n",
    "def write_xml(contours, folder_path,name):\n",
    "    with open(os.path.join(folder_path, name),'w') as f:\n",
    "        f.write('<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n');\n",
    "        f.write('<!-- MIPAV VOI file -->\\n');\n",
    "        f.write('<VOI xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\">\\n')\n",
    "        f.write('<Unique-ID>1234567890</Unique-ID>\\n')\n",
    "        f.write('<Curve-type>0</Curve-type>\\n')\n",
    "        f.write('<Color>255,0,255,0</Color>\\n')\n",
    "        f.write('<Thickness>1</Thickness>\\n')\n",
    "        for i in range(len(contours)):\n",
    "            f.write('<Contour>\\n')\n",
    "            f.write('<Slice-number>0</Slice-number>\\n')\n",
    "            for contour in contours[i]:\n",
    "                f.write('<Pt>')\n",
    "                f.write(str(float(contour[0])))\n",
    "                f.write(',')\n",
    "                f.write(str(float(contour[1])))\n",
    "                f.write('</Pt>\\n')\n",
    "            f.write('</Contour>\\n')\n",
    "        f.write('</VOI>')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#joint_image function\n",
    "e_folders='D:\\\\HCCW\\\\hcc2'\n",
    "_, folders, _ = next(os.walk(e_folders))\n",
    "print(len(folders))\n",
    "for folder in folders: \n",
    "    _,i_folders,_ = next(os.walk(os.path.join(e_folders, folder)))\n",
    "    for image_folders in i_folders:\n",
    "#         _, image_f, _ = next(os.walk(os.path.join(e_folders, folder,image_folders)))\n",
    "#         print(image_f)       \n",
    "        _, hai, _ = next(os.walk(os.path.join(e_folders, folder,image_folders)))\n",
    "        print(haiyue)   \n",
    "        for i in range(len(haiyue)):\n",
    "            _, images, _ = next(os.walk(os.path.join(e_folders, folder,image_folders,hai[i])))\n",
    "            img_joint=[]\n",
    "            label_joint=[]\n",
    "            label_t_joint=[]\n",
    "            label_l_joint=[]\n",
    "            for k in range(len(images)):\n",
    "                full_paths=os.path.join(e_folders, folder,image_folders,hai[i],str(k))\n",
    "                image_path=os.path.join(full_paths,'image.tiff')\n",
    "                label_path=os.path.join(full_paths,'label.png')\n",
    "                img_joint.append(image_path)\n",
    "                label_joint.append(label_path)\n",
    "                if os.path.exists(os.path.join(full_paths,'label_t.xml')):\n",
    "                    label_t_path=os.path.join(full_paths,'label_t.xml')\n",
    "                    label_t_joint.append(label_t_path)\n",
    "                else:\n",
    "                    label_t_joint.append('')\n",
    "                if os.path.exists(os.path.join(full_paths,'label_l.xml')):\n",
    "                    label_l_path=os.path.join(full_paths,'label_l.xml')\n",
    "                    label_l_joint.append(label_l_path)\n",
    "                else:\n",
    "                    label_l_joint.append('')   \n",
    "            region_path=os.path.join(e_folders,folder,image_folders,str(hai[i])[:-1]+'.tiff')\n",
    "            print(region_path)\n",
    "            region = cv2.imread(region_path)\n",
    "#             regions = Image.open(region_path)\n",
    "            n_y, n_x = region.shape[:2]\n",
    "            H=1024\n",
    "            W=1024\n",
    "            num_steps_y = math.ceil(n_y/H)\n",
    "            num_steps_x = math.ceil(n_x/W)\n",
    "            #size n_y,n_x\n",
    "            result_image = Image.new(\"RGB\",(num_steps_x*W,num_steps_y*H))\n",
    "            result_label = Image.new(\"RGB\",(num_steps_x*W,num_steps_y*H))\n",
    "            contours_t=[]\n",
    "            contours_l=[]\n",
    "            for y in range(num_steps_y):\n",
    "                for x in range(num_steps_x):\n",
    "                    image = Image.open(img_joint[num_steps_x*y+x])\n",
    "                    result_image.paste(image,(x *W, y * H))\n",
    "                    label = Image.open(label_joint[num_steps_x*y+x])\n",
    "                    result_label.paste(label,(x *W, y * H))\n",
    "                    if label_t_joint[num_steps_x*y+x]!='':\n",
    "                        contour_tc=xml_contour( label_t_joint[num_steps_x*y+x])\n",
    "                        contour_t=change_number(contour_tc,x,y)\n",
    "                        for j in range(len(contour_t)):\n",
    "                            contours_t.append(contour_t[j])\n",
    "                    if label_l_joint[num_steps_x*y+x]!='':\n",
    "                        contour_lc=xml_contour( label_l_joint[num_steps_x*y+x])\n",
    "                        contour_l=change_number(contour_lc,x,y)\n",
    "                        for l in range(len(contour_l)):\n",
    "                            contours_l.append(contour_l[l])        \n",
    "                        \n",
    "            folder_path=os.path.join(e_folders,folder,image_folders,hai[i])        \n",
    "            write_xml(contours_t,folder_path,\"label_t.xml\")    \n",
    "            write_xml(contours_l,folder_path,\"label_l.xml\") \n",
    "            result_image_path=os.path.join(e_folders,folder,image_folders,hai[i],\"image.tiff\")\n",
    "            result_label_path=os.path.join(e_folders,folder,image_folders,hai[i],\"label.tiff\")\n",
    "            result_image.save(result_image_path)     \n",
    "            result_label.save(result_label_path)\n",
    "print(\"finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_xml_contour(xml_path, height, width):\n",
    "    '''\n",
    "    read contour from xml file and return mask\n",
    "    Args:\n",
    "        xml_path: full path of the xml file\n",
    "        height: height of the image\n",
    "        width: width of the image\n",
    "    Return:\n",
    "        mask: np array, shape=(height, width, num_instances), dtype=bool\n",
    "    '''\n",
    "    with open(xml_path, 'r') as fp:\n",
    "        soup = BeautifulSoup(fp.read(), 'xml')\n",
    "    contours = []\n",
    "    for contour in soup.find_all('Contour'):\n",
    "        points = []\n",
    "        for pt in contour.find_all('Pt'):\n",
    "            points.append(tuple(float(v) for v in pt.get_text().split(',')))\n",
    "        contours.append(points)\n",
    "    masks = []\n",
    "    for contour in contours:\n",
    "        img = Image.new('L', (width, height), 0)\n",
    "        ImageDraw.Draw(img).polygon(contour, outline=1, fill=1)\n",
    "        masks.append(np.array(img))   \n",
    "    mask = np.stack(masks, axis=-1)\n",
    "    return mask.astype(bool) # mask should be boolean type\n",
    "\n",
    "def load_image(folder_path):\n",
    "    image_path =  os.path.join(folder_path,\"image.tiff\")\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
    "    image = normalizeStaining(image)\n",
    "    return image\n",
    "def load_mask(folder_path):\n",
    "    height=1024\n",
    "    width=1024\n",
    "    MASK_T=[]\n",
    "    MASK_L=[]\n",
    "    class_t=[]\n",
    "    class_l=[]\n",
    "    if os.path.exists(os.path.join(folder_path,'label_t.xml')):\n",
    "        label_t_path = os.path.join(folder_path,'label_t.xml')\n",
    "        mask_t = read_xml_contour(label_t_path, height, width)\n",
    "        class_ids_t = np.ones(mask_t.shape[2], dtype=int)\n",
    "        MASK_T= mask_t\n",
    "        class_t=class_ids_t\n",
    "    else:\n",
    "        class_ids_t=[]\n",
    "        class_t=class_ids_t\n",
    "    if os.path.exists(os.path.join(folder_path,'label_l.xml')):\n",
    "        label_l_path = os.path.join(folder_path,'label_l.xml')\n",
    "        mask_l = read_xml_contour(label_l_path, height, width)\n",
    "        q=mask_l.shape[2]\n",
    "        class_ids_l=np.zeros(mask_l.shape[2],dtype=int)\n",
    "        for i in range(q):\n",
    "            class_ids_l[i]=2 \n",
    "        MASK_L=mask_l\n",
    "        class_l=class_ids_l    \n",
    "    else:\n",
    "        class_ids_l=[]   \n",
    "        class_l=class_ids_l    \n",
    "    \n",
    "    return  MASK_T,class_t,MASK_L,class_l       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cell number\n",
    "def ca_number_tumor(folder_path):\n",
    "    _,image_path,_=next(os.walk(folder_path))\n",
    "    class_t=[]\n",
    "    class_l=[]\n",
    "    for i in range(len(image_path)):\n",
    "        f_path=os.path.join(folder_path,image_path[i])\n",
    "        mask_t,class_ids_t,mask_l,class_ids_l =load_mask(f_path)\n",
    "        for j in range(len(class_ids_t)):\n",
    "            class_t.append(class_ids_t[j])\n",
    "        for k in range(len(class_ids_l)):\n",
    "            class_l.append(class_ids_l[k])    \n",
    "    return class_t,class_l        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relation(folder_path,board_arg):\n",
    "    region_path = os.path.join(folder_path,\"image.tiff\")\n",
    "    region = cv2.imread(region_path)\n",
    "#     regions = Image.open(region_path)\n",
    "    n_y, n_x = region.shape[:2]\n",
    "    t=[]\n",
    "    l=[]\n",
    "    _,image_path,_=next(os.walk(folder_path))\n",
    "    for i in range(len(image_path)):\n",
    "        label_t_path=os.path.join(folder_path,image_path[i],\"label_t.xml\")\n",
    "        label_l_path=os.path.join(folder_path,image_path[i],\"label_l.xml\")\n",
    "        if os.path.exists(label_l_path):\n",
    "            mask_l = read_xml_contour(label_l_path, 1024,1024)\n",
    "            mask_image_l = utilities.mask_to_label(mask_l)\n",
    "        else:\n",
    "            mask_image_l=[]\n",
    "        if os.path.exists(label_t_path):\n",
    "            mask_t = read_xml_contour(label_t_path, 1024, 1024)\n",
    "            mask_image_t = utilities.mask_to_label(mask_t)            \n",
    "            for i in range(mask_t.shape[2]):\n",
    "                _, contour, _ = cv2.findContours(mask_t[...,i].astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "                x,y,w,h = coordinate_regions(contour)\n",
    "                x_c,x_z,y_c,y_z=boardcast_coor(board_arg,x,y,w,h)\n",
    "                if x_c>=0 and y_c>=0 and x_z>=1024 and y_z>=1024:\n",
    "                    croped_t = mask_image_t[y_c:y_z,x_c:x_z]\n",
    "                    croped_t=utilities.label_to_mask(croped_t)\n",
    "                    if mask_image_l!=[]:\n",
    "                        croped_l = mask_image_l[y_c:y_z,x_c:x_z]\n",
    "                        croped_l=utilities.label_to_mask(croped_l)\n",
    "                        l.append(croped_l.shape[2])\n",
    "                    t.append(croped_t.shape[2])\n",
    "                \n",
    "    return t,l    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#density\n",
    "def density(folder_path):\n",
    "    region_path = os.path.join(folder_path,\"image.tiff\")\n",
    "    region =cv2.imread(region_path)\n",
    "    gray = cv2.cvtColor(region,cv2.COLOR_BGR2GRAY)\n",
    "    _,contours, _ = cv2.findContours(gray,cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "    cnt = contours[0]\n",
    "    area = cv2.contourArea(cnt)\n",
    "    class_t,class_l= ca_number_tumor(folder_path)\n",
    "    tumor_number=len(class_t)\n",
    "    lyphocyte_number=len(class_l)\n",
    "    density_tumor=tumor_number/area\n",
    "    density_lyphocyte=lyphocyte_number/area\n",
    "    return density_tumor,density_lyphocyte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uniformity\n",
    "def num_matrix(folder_path):\n",
    "    region_path=os.path.join(folder_path,\"image.tiff\")\n",
    "    _,image_path,_=next(os.walk(folder_path))\n",
    "    print(image_path)\n",
    "    region = cv2.imread(region_path)\n",
    "#     regions = Image.open(region_path)\n",
    "    n_y, n_x = region.shape[:2]\n",
    "    H=1024\n",
    "    W=1024\n",
    "    num_steps_y = math.ceil(n_y/H)\n",
    "    num_steps_x = math.ceil(n_x/W)\n",
    "    t_n=[]\n",
    "    l_n=[]\n",
    "    for y in range(num_steps_y):\n",
    "        row_t=[]\n",
    "        row_l=[]\n",
    "        for x in range(num_steps_x):\n",
    "            img_path=os.path.join(folder_path,image_path[num_steps_x*y+x])\n",
    "            mask_t,class_ids_t,mask_l,class_ids_l =load_mask(img_path)\n",
    "            row_t.append(len(class_ids_t))\n",
    "            row_l.append(len(class_ids_l))\n",
    "        t_n.append(row_t)    \n",
    "        l_n.append(row_l)\n",
    "    t_n=np.array(t_n)   \n",
    "    l_n=np.array(l_n) \n",
    "    return t_n,l_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unifority(t_n,l_n):\n",
    "    tumor_mean=np.mean(t_n)\n",
    "    lyphocyte_mean=np.mean(l_n)\n",
    "    tumor_std=np.std(t_n)\n",
    "    lyphocyte_std=np.std(l_n)\n",
    "    \n",
    "    tumor_var=np.var(t_n)\n",
    "    tumor_median=np.median(t_n)\n",
    "    tumor_max=np.max(t_n)\n",
    "    tumor_min=np.min(t_n)\n",
    "    tumor_ptp=np.ptp(t_n)\n",
    "    tumor_cov=np.min(t_n)\n",
    "    lyphocyte_var=np.var(l_n)\n",
    "    lyphocyte_median=np.median(l_n)\n",
    "    lyphocyte_max=np.max(l_n)\n",
    "    lyphocyte_min=np.min(l_n)  \n",
    "    lyphocyte_ptp=np.ptp(l_n)\n",
    "    lyphocyte_cov=np.min(l_n)    \n",
    "    return tumor_mean,lyphocyte_mean,tumor_std,lyphocyte_std,tumor_var,lyphocyte_var,tumor_median,lyphocyte_median,tumor_max,lyphocyte_max,tumor_min,lyphocyte_min,tumor_ptp,lyphocyte_ptp,tumor_cov,lyphocyte_cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boardcast_coor(ag,x,y,w,h):\n",
    "    x_c=x-ag\n",
    "    x_z=x+w+ag\n",
    "    y_c=y-ag\n",
    "    y_z=y+h+ag\n",
    "    return x_c,x_z,y_c,y_z"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
